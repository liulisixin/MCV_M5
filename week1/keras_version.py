# -*- coding: utf-8 -*-
"""wee3_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k4WD9cEfUEF70psC1a2RrhbLywoXn6np
"""

from keras.applications.resnet_v2 import ResNet50V2
from keras.applications.resnet_v2 import preprocess_input as preprocess_resnetV2
from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D
from keras.layers import Conv2D, MaxPooling2D, Add, BatchNormalization, ReLU
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.callbacks import ReduceLROnPlateau
from keras.optimizers import Adam, SGD
from keras import metrics
from keras.engine.topology import Layer
from keras.utils import plot_model


# Define the residual block as a new layer
class Residual(Layer):
    def __init__(self, channels_in,kernel,**kwargs):
        super(Residual, self).__init__(**kwargs)
        self.channels_in = channels_in
        self.kernel = kernel

    def call(self, x):
        # the residual block using Keras functional API
        first_layer =   Activation("linear", trainable=False)(x)
        x =             Conv2D( self.channels_in,
                                self.kernel,
                                padding="same")(first_layer)
        x =             Activation("relu")(x)
        x =             Conv2D( self.channels_in,
                                self.kernel,
                                padding="same")(x)
        residual =      Add()([x, first_layer])
        x =             Activation("relu")(residual)
        return x

    def compute_output_shape(self, input_shape):
        return input_shape


def build_model():
    # model = Sequential([Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False, input_shape=(256, 256, 3)),
    #                     BatchNormalization(),
    #                     ReLU(),
    #                     Conv2D(8, kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False),
    #                     BatchNormalization(),
    #                     ReLU(),
    #                     GlobalAveragePooling2D()])

    # model = Sequential([Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False, input_shape=(256, 256, 3)),
    #                 BatchNormalization(),
    #                 ReLU(),
    #                 Conv2D(32, kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False),
    #                 BatchNormalization(),
    #                 ReLU(),
    #                 Conv2D(64, kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False),
    #                 BatchNormalization(),
    #                 ReLU(),
    #                 Conv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False),
    #                 BatchNormalization(),
    #                 ReLU(),
    #                 GlobalAveragePooling2D()])

    model = Sequential(
        [Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False, input_shape=(256, 256, 3)),
         BatchNormalization(),
         MaxPooling2D(),
         ReLU(),
         Conv2D(32, kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False),
         BatchNormalization(),
         ReLU(),
         Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False),
         BatchNormalization(),
         ReLU(),
         Conv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False),
         BatchNormalization(),
         MaxPooling2D(),
         ReLU(),
         GlobalAveragePooling2D()])

    # model2 = Sequential([Conv2D(64, (3, 3), padding='same', input_shape=(224, 224, 3)), Residual(64, (1, 1)), Conv2D(8, (3, 3), padding='same'), layers.GlobalAveragePooling2D()])

    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)
    return model
    



def random_search(num_iters, train_dir, epochs, parameters):
    all_res = []
    best_acc = 0
    best_app = 0
    best_parameters = {}
    
    for i in range(num_iters):

        params = select_random_parameters(parameters)
        print("Running With:")
        print(params)

        train_loader = build_dataloader(train_dir, True, params)
        test_loader = build_dataloader("../MIT_split/test", False, params)
        model = build_model()

        results = tune_model(model, epochs, params, train_loader, test_loader)

        all_res.append(results)
        if max(all_res[-1].history["val_acc"]) > best_acc:
            best_parameters = params
            best_acc = max(all_res[-1].history["val_acc"])
            best_app = max(all_res[-1].history["val_app"])
        print(best_acc)
        print(best_app)
        print(best_parameters)
    
    print(best_acc)
    print(best_app)
    print("BEST PARAMETERS: {}".format(best_parameters))

    return all_res


def tune_model(model, epochs, parameters, train_loader, val_loader, fname="model.pth"):
    
    if "Adam" in parameters["optimizer"]:
        optimizer = Adam(lr=parameters["lr"], decay=parameters["decay"])
    else:
        optimizer = SGD(lr=parameters["lr"], momentum=parameters["momentum"], decay=parameters["decay"])

    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=["accuracy", app])
    hist = model.fit_generator(train_loader, epochs=epochs, validation_data=val_loader,
                                  steps_per_epoch=int(train_loader.samples // parameters['batch_size']), callbacks=[parameters["callback"]])


    return hist




def build_dataloader(root_dir, is_train=False, parameters=None):
    if is_train:
        assert parameters is not None, "Parameters must be defined when building train dataloader."

        # data_generator = ImageDataGenerator(featurewise_center=parameters['feature_center'],
        #                                     samplewise_center=parameters['sample_center'],
        #                                     featurewise_std_normalization=parameters['feature_norm'],
        #                                     samplewise_std_normalization=parameters['sample_norm'],
        #                                     preprocessing_function=preprocess_resnetV2,
        #                                     rotation_range=parameters['rotation'],
        #                                     width_shift_range=parameters['width_padding'],
        #                                     height_shift_range=parameters['height_padding'],
        #                                     shear_range=parameters['shear'],
        #                                     zoom_range=parameters['zoom'],
        #                                     fill_mode=parameters['fill_mode'],
        #                                     horizontal_flip=parameters['hflip'],
        #                                     vertical_flip=parameters['vflip'],
        #                                     rescale=parameters['rescale'])
        #data_generator = ImageDataGenerator(preprocessing_function=preprocess_resnetV2)
        data_generator = ImageDataGenerator(rescale=parameters['rescale'],
                           zoom_range=parameters['zoom'],
                           shear_range=parameters['shear'],
                           horizontal_flip=parameters['hflip'])

    else:
        data_generator = ImageDataGenerator(rescale=parameters['rescale'])

    split_generator = data_generator.flow_from_directory(root_dir,
                              classes=['coast', 'forest', 'highway', 'inside_city',
                              'mountain', 'Opencountry', 'street', 'tallbuilding'],
                              target_size=(256, 256),
                              batch_size = parameters['batch_size'])

    return split_generator

def app(y_true, y_pred):
    accuracy = metrics.categorical_accuracy(y_true, y_pred)

    return accuracy / (model.count_params() / 1e6)

from random import choice
import numpy as np
def select_random_parameters(parameters):
    random_params = {}
    for parameter, values in parameters.items():
        if isinstance(values, list):
            random_params[parameter] = choice(values)
        elif isinstance(values, tuple):
            random_params[parameter] = np.random.uniform(values[0], values[1])
        
            if parameter == "momentum":
                random_params[parameter] = 1.05**random_params[parameter]

            elif parameter == "lr" or parameter == "decay":
                random_params[parameter] = 10**random_params[parameter]

        else:
            assert "Not a valid type"
    return random_params

import math
from keras.callbacks import Callback
from keras import backend as K


class CosineAnnealingScheduler(Callback):
    """Cosine annealing scheduler.
    """

    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):
        super(CosineAnnealingScheduler, self).__init__()
        self.T_max = T_max
        self.eta_max = eta_max
        self.eta_min = eta_min
        self.verbose = verbose

    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2
        K.set_value(self.model.optimizer.lr, lr)
        if self.verbose > 0:
            print('\nEpoch %05d: CosineAnnealingScheduler setting learning '
                  'rate to %s.' % (epoch + 1, lr))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)



# train_loader = build_dataloader("../MIT_split/train")
# test_loader = build_dataloader("../MIT_split/test")

model = build_model()
# callbacks = CosineAnnealingScheduler(T_max=30, eta_max=0.05, eta_min=4e-4)
#parameters = {
#        # MODEL
#        "batch_size": [16, 32, 64],
#        "optimizer": ["SGD", "Adam"],
#        "lr": (0.001, 0.005),
#        "momentum": (0.0, 0.99),
#        "decay": (1e-3, 1e-5),
#        "callback": [callbacks],}

model.count_params()

'''
#parameters = {
#        # MODEL
#        "batch_size": 128,
#        "optimizer": ["SGD"],
#        "lr": 0.001,
#        "momentum": 0.90,
#        "decay": 1e-5,
#        "callback": [callbacks],}

callbacks = CosineAnnealingScheduler(T_max=30, eta_max=0.05, eta_min=4e-4)
parameters = {
        # MODEL, 
        "batch_size": [16, 32, 64],
        "optimizer": ["SGD", "Adam"],
        "lr": (-1, -5),   #will 10**
        "momentum": (-1, -3), #will 1.05**
        "decay": (-3, -5),  #will 10**

        "rescale": [1.0, 1.0/128, 1.0/255],
        "zoom": (0, 0.3),
        "shear": (0, 0.3),
        "hflip": [True, False],
        "callback": [callbacks],}

all_hist = random_search(20, "../MIT_split/train", 40, parameters)
#hist = tune_model(model, 50, parameters, train_loader, test_loader)
'''

import matplotlib.pyplot as plt
def plot_history(history,title = "pic_"):
    # summarize history for accuracy
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.savefig(title+'accuracy.png')
    plt.close()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.savefig(title+'loss.png')
    plt.close()

#run with one set of parameters
callbacks = CosineAnnealingScheduler(T_max=30, eta_max=0.05, eta_min=4e-4)
params = {
        # MODEL, 
        "batch_size": 16,
        "optimizer": "SGD",
        "lr": 4.300e-05,
        "momentum": 0.879961,
        "decay": 8.946e-05,

        "rescale": 1.0,
        "zoom": 0.077509,
        "shear": 0.000586,
        "hflip": True,
        "callback": callbacks,}

print("Running With:")
print(params)
epochs = 40

train_loader = build_dataloader("../MIT_split/train", True, params)
test_loader = build_dataloader("../MIT_split/test", False, params)
model = build_model()

results = tune_model(model, epochs, params, train_loader, test_loader)

plot_history(results, "hist_post")
print("Loss_train: {} ".format(min(results.history["loss"])))
print("Loss_test: {} ".format((min(results.history["val_loss"]))))
print("Accuracy_train: {} ".format(max(results.history["acc"])))
print("Accuracy_test: {} ".format(max(results.history["val_acc"])))
